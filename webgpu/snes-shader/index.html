<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SNES shader</title>
  <link id="shader" rel="preload" type="text/wgsl" href="./shaders/shader.wgsl" as="fetch" crossorigin="anonymous">
  <style>
    body {
      background-color: #B9B6AC;
      color: #374341;
      font-family: Verdana, Geneva, Tahoma, sans-serif;
      font-style: italic;
    }

    canvas, img {
      /* Make sure the browser doesn't do any resampling */
      image-rendering: pixelated;
    }

    div.error {
      position: fixed;
      display: none;
      left: 1em;
      right: 1em;
      border: solid 4px red;
      background-color: black;
      color: white;
      padding: 1em
    }

    #displayArea canvas {
      transition: filter 1.5s;
      filter: none;
    }

    #displayArea.loading canvas {
      filter: brightness(0);
    }

    #displayArea figure {
      display: inline-block;
    }

    #displayArea figure > div {
      height: 672px; /* 224 x 3 */
      background-color: black;
      border: solid 1px black;
      overflow: hidden;
    }

    #examples {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
    }

    #examples img {
      /* This is the standard resolution of the SNES */
      width: 256px;
      height: 224px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <!--
    NTSC has 240 scanlines, but the horizontal resolution is entirely dependent on how quickly
    the signal can be modulated in the 52.148 microsecond active period of the scanline.

    The SNES S-PPU had a 5.37 MHz clock-rate, which meant that it could fit 280 pixels into
    each horizontal scanline. The Super NES fills 256 of these with image data and leaves the
    remainder that fall into the overscan area black.

    Fitting a 280 x 240 image to 4:3 aspect ratio requires stretching the pixels horizontally
    by a factor of 8/7.

    While the SNES can almost fill all 240 scanlines, it's traditional to crop the image to
    224 lines as an approximation of what the TV cuts off. This is where the 256 x 224 pixel
    resolution of most SNES screenshots comes from.

    Scaling up by 3x and stretching the pixels by 8/7 gives us an approximate image resolution
    of 878 x 672 pixels [(256 * (8/7) * 3) x (224 * 3)].

    Reference: https://gaming.stackexchange.com/a/318959
  -->
  <h1><span style="color: #00926A">S</span><span style="color: #014FA4">N</span><span style="color: #FFBC56">E</span><span style="color: #DE3A48">S</span> shader</h1>

  <div id="not-supported" class="error">
    <h2>Oh snap!</h2>
    <p>Your browser does not seem to support the required <a href="https://caniuse.com/webgpu">WebGPU</a> or <a href="https://caniuse.com/webcodecs">WebCodecs</a> features. Please try updating to Chrome 119 (Desktop) or another recent browser release.</p>
  </div>

  <section id="displayArea">
    <figure>
      <div class="border">
        <canvas id="filteredImage" width="878" height="672">Canvas not supported</canvas>
      </div>
      <figcaption>Filtered (scaled 3x)</figcaption>
    </figure>
    <figure>
      <div class="border">
        <canvas id="originalImage" width="768" height="672">Canvas not supported</canvas>
      </div>
      <figcaption>Original (scaled 3x)</figcaption>
    </figure>
  </section>

  <h2>Examples</h2>
  <section id="examples">
    <a href="#dkc2-klobber-karnage"><img src="resources/dkc2-klobber-karnage.png"></a>
    <a href="#chrono-trigger-fiendlord's-keep"><img src="resources/chrono-trigger-fiendlords-keep.png"></a>
    <a href="#chrono-trigger-lucca's-great-invention"><img src="resources/chrono-trigger-luccas-great-invention.png"></a>
    <a href="#street-fighter-2-hadouken"><img src="resources/street-fighter-2-hadouken.png"></a>
    <a href="#starfox"><img src="resources/starfox.png"></a>
    <a href="#starfox2"><img src="resources/starfox2.png"></a>
    <a href="#super-mario-all-stars-world"><img src="resources/super-mario-all-stars-world.png"></a>
    <a href="#tales-of-phantasia"><img src="resources/tales-of-phantasia.png"></a>
    <a href="#breath-of-fire"><img src="resources/breath-of-fire.png"></a>
    <a href="#the-lion-king"><img src="resources/the-lion-king.png"></a>
    <a href="#dkc3-gameover"><img src="resources/dkc3-gameover.png"></a>
    <a href="#yoshis-island"><img src="resources/yoshis-island.gif"></a>
    <a href="#secret-of-mana-2"></a><img src="resources/secret-of-mana-2.png"></a>
    <a href="#star-ocean"><img src="resources/star-ocean.png"></a>
  </div>

  <script type="module">
    const DEFAULT = '#klobber-karnage';
    const RESOLUTION = [256, 224];
    const FPS = 25;

    // Render quad: array<vec2<f32>>
    const quad = new Float32Array([
      // upper triangle
      0.0, 1.0,
      1.0, 0.0,
      1.0, 1.0,

      // lower triangle
      0.0, 0.0,
      1.0, 0.0,
      0.0, 1.0,
    ]);

    // Uniform data
    const uniformData = new Float32Array([
      // viewProjection: mat4x4<f32> (fill viewport)
      2.0, 0.0, 0.0, 0.0,
      0.0, 2.0, 0.0, 0.0,
      0.0, 0.0, 2.0, 0.0,
      -1.0, -1.0, 0.0, 1.0,
    ]);

    // Draw indices for vertex ordering
    const indices = new Uint16Array([0, 1, 2, 3, 4, 5]);

    /**
     * Main function
     */
    async function main() {
      const adapter = await navigator.gpu?.requestAdapter();
      const device = await adapter?.requestDevice({
        requiredLimits: {},
        requiredFeatures: [],
      });

      if (!device) {
        document.getElementById('not-supported').style.display = 'block';
        console.log('WebGPU not supported');
        return;
      }

      if (location.hash.length < 2) {
        // Load default image
        location.hash = DEFAULT;
      }

      const displayArea = document.getElementById('displayArea');

      const originalImage = document.getElementById('originalImage');
      const originalContext = originalImage.getContext('2d');

      const filteredImage = document.getElementById('filteredImage')
      const context = filteredImage.getContext('webgpu');
      const queue = device.queue;

      // Helper for creating GPU buffers
      function createBuffer(arr, usage) {
        const buffer = device.createBuffer({
          size: (arr.byteLength + 3) & ~3,
          usage,
          mappedAtCreation: true,
        });

        const writeArray = arr instanceof Uint16Array ? new Uint16Array(buffer.getMappedRange()) : new Float32Array(buffer.getMappedRange());
        writeArray.set(arr);
        buffer.unmap();

        return buffer;
      }

      // Helper for loading shaders
      async function loadShader(url) {
        const result = await fetch(url);
        if (!result.ok) {
          throw new Error(`Unable to fetch ${url}: HTTP ${result.status} ${result.statusText}`);
        }

        const shaderCode = await result.text();

        return device.createShaderModule({ label: url, code: shaderCode });
      }

      // Fetch an image from a URL and wrap in an ImageDecoder.
      // This allows access to multiple frames of an animated GIF (Image only gives you frame 0).
      // Unfortunately this requires Chrome Desktop 119 or newer.
      async function fetchDecodeableImage(url) {
        const result = await fetch(url);
        const content_type = result.headers.get('Content-Type');

        const decoder = new ImageDecoder({
          type: content_type,
          data: result.body,
          colorSpaceConversion: 'none',
          preferAnimation: true,
        });

        await decoder.completed;

        return decoder;
      }

      const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
      context.configure({
        device: device,
        format: presentationFormat,
        usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_SRC,
        alphaMode: 'opaque',
      });

      // Buffers for rendering quad
      const positionBuffer = createBuffer(quad, GPUBufferUsage.VERTEX);
      const indexBuffer = createBuffer(indices, GPUBufferUsage.INDEX);
      const uniformBuffer = createBuffer(uniformData, GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST);

      // Intermediate texture for filtering
      const intermediateTexture = device.createTexture({
        format: 'rgba8unorm',
        size: RESOLUTION,
        usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.RENDER_ATTACHMENT,
      });

      const shaderPreload = document.getElementById('shader');
      const shaderModule = await loadShader(shaderPreload.href);

      // This pipeline does the horizontal chroma subsampling
      const pipeline1 = device.createRenderPipeline({
        layout: "auto",
        vertex: {
          module: shaderModule,
          entryPoint: 'vertShader',
          buffers: [
            // position
            {
              attributes: [
                {
                  shaderLocation: 0, // @location(0)
                  offset: 0,
                  format: 'float32x2',
                }
              ],
              arrayStride: 4 * 2, // sizeof(float) * 2
              stepMode: 'vertex',
            },
          ],
        },
        fragment: {
          module: shaderModule,
          entryPoint: 'fragShader1',
          targets: [
            { format: intermediateTexture.format },
          ],
        },
        primitive: {
          frontFace: 'cw',
          cullMode: 'none',
          topology: 'triangle-list',
        },
      });

      // This pipeline applies chromatic abberation
      const pipeline2 = device.createRenderPipeline({
        layout: "auto",
        vertex: {
          module: shaderModule,
          entryPoint: 'vertShader',
          buffers: [
            // position
            {
              attributes: [
                {
                  shaderLocation: 0, // @location(0)
                  offset: 0,
                  format: 'float32x2',
                }
              ],
              arrayStride: 4 * 2, // sizeof(float) * 2
              stepMode: 'vertex',
            },
          ],
        },
        fragment: {
          module: shaderModule,
          entryPoint: 'fragShader2',
          targets: [
            { format: presentationFormat },
          ],
        },
        primitive: {
          frontFace: 'cw',
          cullMode: 'none',
          topology: 'triangle-list',
        },
      });

      const bindGroup2 = device.createBindGroup({
        layout: pipeline2.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: device.createSampler({ magFilter: 'linear' }) },
          { binding: 1, resource: intermediateTexture.createView() },
          { binding: 2, resource: { buffer: uniformBuffer }},
        ],
      });

      let currentURL = null;
      let imageDecoder = null;

      // Called once per frame
      async function render(timestamp) {
        const element = document.querySelector(`a[href="${location.hash}"] img`);

        if (element && currentURL != element.src) {
          displayArea.classList.add('loading');

          if (imageDecoder) {
            imageDecoder.close();
          }

          // Only change the decoder in the render function to avoid race conditions
          imageDecoder = await fetchDecodeableImage(element.src);
          currentURL = element.src;
        }

        if (!imageDecoder || !imageDecoder.complete || !imageDecoder.tracks.selectedTrack) {
          // Decoder isn't ready yet, so reschedule
          requestAnimationFrame(render);
          return;
        }

        displayArea.classList.remove('loading');

        // What frame should we display?
        const n = Math.floor(FPS * timestamp / 1000.0) % imageDecoder.tracks.selectedTrack.frameCount;
        const frame = await imageDecoder.decode({ frameIndex: n });

        // Draw the source frame to a Canvas 2D
        originalContext.imageSmoothingEnabled = false;
        originalContext.drawImage(frame.image, 0, 0, 768, 672);

        const externalTexture = device.importExternalTexture({ source: frame.image });

        const bindGroup1 = device.createBindGroup({
          layout: pipeline1.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: device.createSampler() },
            { binding: 2, resource: { buffer: uniformBuffer }},
            { binding: 3, resource: externalTexture },
          ],
        });

        // Command buffer
        const commandEncoder = device.createCommandEncoder();

        // This pass performs chroma subsampling
        const pass1 = commandEncoder.beginRenderPass({
          colorAttachments: [
            {
              view: intermediateTexture.createView(),
              clearValue: { r: 0, g: 0, b: 0, a: 1 },
              loadOp: 'clear',
              storeOp: 'store',
            },
          ],
        });
        pass1.setPipeline(pipeline1);
        pass1.setBindGroup(0, bindGroup1);
        pass1.setVertexBuffer(0, positionBuffer);
        pass1.setIndexBuffer(indexBuffer, 'uint16');
        pass1.drawIndexed(6);
        pass1.end();

        // This pass applies chromatic abberation and renders to the canvas
        const pass2 = commandEncoder.beginRenderPass({
          colorAttachments: [
            {
              view: context.getCurrentTexture().createView(),
              clearValue: { r: 0, g: 0, b: 0, a: 1 },
              loadOp: 'clear',
              storeOp: 'store',
            },
          ],
        });
        pass2.setPipeline(pipeline2);
        pass2.setBindGroup(0, bindGroup2);
        pass2.setVertexBuffer(0, positionBuffer);
        pass2.setIndexBuffer(indexBuffer, 'uint16');
        pass2.drawIndexed(6);
        pass2.end();

        queue.submit([ commandEncoder.finish() ]);

        requestAnimationFrame(render);
      }

      // Start rendering
      requestAnimationFrame(render);
    }

    window.addEventListener('load', () => {
      // Just rethrow any async error
      main().catch(err => { throw err; });
    })
  </script>
</body>
</html>
